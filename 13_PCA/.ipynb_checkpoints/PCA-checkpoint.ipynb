{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f51119b-39a0-4579-8146-0d9348f66005",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "Curse of Dimensionality in Machine Learning arises when working with high-dimensional data, leading to increased computational complexity, overfitting, and spurious correlations.\n",
    "\n",
    "In high-dimensional spaces, data points become sparse, making it challenging to discern meaningful patterns or relationships due to the vast amount of data required to adequately sample the space.\n",
    "\n",
    "### Dimensionality Reduction Techniques:\n",
    "- **Feature Selection:** Identify and select the most relevant features from the original dataset while discarding irrelevant or redundant ones. This reduces the dimensionality of the data, simplifying the model and improving its efficiency.\n",
    "- **Feature Extraction:** Transform the original high-dimensional data into a lower-dimensional space by creating new features that capture the essential information. Techniques such as Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are commonly used for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676480e-87b7-4d06-afe2-916937286928",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "It used to further reduce the dimensionality of the selected features. It transforms the data into a lower-dimensional space while retaining as much variance as possible. We will keep the columns which has the most variance.\n",
    "\n",
    "It changes your original features into new features these new features donâ€™t overlap with each other\n",
    "\n",
    "It finds these by calculating eigenvectors (directions) and eigenvalues (importance) from the covariance matrix. PCA selects the top components with the highest eigenvalues and projects the data onto them simplify the dataset.\n",
    "\n",
    "### Steps for PCA\n",
    "**Step 1: Standardize the Data**\n",
    "Different features may have different units and scales like salary vs. age. To compare them fairly PCA first standardizes the data by making each feature have Zero mean\n",
    "\n",
    "**Step 2: Calculate Covariance Matrix**\n",
    "Next PCA calculates the covariance matrix to see how features relate to each other whether they increase or decrease together.\n",
    "\n",
    "**Step 3: Find the Principal Components**\n",
    "PCA identifies new axes where the data spreads out the most:\n",
    "\n",
    "1st Principal Component (PC1): The direction of maximum variance (most spread).\n",
    "2nd Principal Component (PC2): The next best direction, perpendicular to PC1 and so on.\n",
    "These directions come from the eigenvectors of the covariance matrix and their importance is measured by eigenvalues. For a square matrix A an eigenvector X (a non-zero vector) and its corresponding eigenvalue Î» satisfy:\n",
    "\n",
    "AX=Î»X\n",
    "\n",
    "This means:\n",
    "\n",
    "When A acts on X it only stretches or shrinks X by the scalar Î».\n",
    "The direction of X remains unchanged hence eigenvectors define \"stable directions\" of A.\n",
    "Eigenvalues help rank these directions by importance.\n",
    "\n",
    "**Step 4: Pick the Top Directions & Transform Data**\n",
    "After calculating the eigenvalues and eigenvectors PCA ranks them by the amount of information they capture. We then:\n",
    "\n",
    "Select the top k components hat capture most of the variance like 95%.\n",
    "Transform the original dataset by projecting it onto these top components.\n",
    "This means we reduce the number of features (dimensions) while keeping the important patterns in the data.\n",
    "\n",
    "**Visualizing Eigen vectors and Eigen values** <a>https://www.geogebra.org/m/YCZa8TAH</a>\n",
    "\n",
    "**A geometric interpretation of the covariance matrix** <br>\n",
    "<a>https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#:~:text=covariance%20matrix%20captures%20the%20spread%20of%20N%2Ddimensional%20data.&text=Figure%203.,is%20captured%20by%20the%20variance</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2341799-6c71-43af-952d-6c4e0778bc63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f682f8f-9270-42ac-a7a0-b6f056278fbd",
   "metadata": {},
   "source": [
    "## ðŸ“Š Covariance and Covariance Matrix in Machine Learning\n",
    "\n",
    "### ðŸ”¹ What is Covariance?\n",
    "Covariance measures how **two variables vary together** â€” whether they increase or decrease in relation to each other.\n",
    "\n",
    "\n",
    "### ðŸ” Interpretation\n",
    "| Covariance Value | Interpretation |\n",
    "|------------------:|---------------|\n",
    "| \\( > 0 \\) | \\( X \\) and \\( Y \\) increase together (positive relationship) |\n",
    "| \\( < 0 \\) | \\( X \\) increases when \\( Y \\) decreases (negative relationship) |\n",
    "| \\( = 0 \\) | No linear relationship between \\( X \\) and \\( Y \\) |\n",
    "\n",
    "> Covariance shows **direction of relationship**, but not its **strength** (unlike correlation, which is normalized between -1 and 1).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Covariance Matrix\n",
    "we could calculate the variance \\sigma(x,x) in the x-direction and the variance \\sigma(y,y) in the y-direction. However, the horizontal spread and the vertical spread of the data does not explain the clear diagonal correlation. Figure 2 clearly shows that on average, if the x-value of a data point increases, then also the y-value increases, resulting in a positive correlation. This correlation can be captured by extending the notion of variance to what is called the â€˜covarianceâ€™ of the data:\n",
    "\n",
    "(2) \\begin{equation*} \\sigma(x,y) = \\mathbb{E}[ (x - \\mathbb{E}(x)) (y - \\mathbb{E}(y))] \\end{equation*}\n",
    "\n",
    "For 2D data, we thus obtain \\sigma(x,x), \\sigma(y,y), \\sigma(x,y) and \\sigma(y,x). These four values can be summarized in a matrix, called the covariance matrix:\n",
    "\n",
    "(3) \\begin{equation*} \\Sigma = \\begin{bmatrix} \\sigma(x,x) & \\sigma(x,y) \\\\[0.3em] \\sigma(y,x) & \\sigma(y,y) \\\\[0.3em] \\end{bmatrix} \\end{equation*}\n",
    "\n",
    "If x is positively correlated with y, y is also positively correlated with x. In other words, we can state that \\sigma(x,y) = \\sigma(y,x). Therefore, the covariance matrix is always a symmetric matrix with the variances on its diagonal and the covariances off-diagonal. Two-dimensional normally distributed data is explained completely by its mean and its 2\\times 2 covariance matrix. Similarly, a 3 \\times 3 covariance matrix is used to capture the spread of three-dimensional data, and a N \\times N covariance matrix captures the spread of N-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Intuition in ML\n",
    "- Covariance measures **how features move together** â€” crucial for understanding **feature redundancy** and **correlation**.\n",
    "- Large covariances between features â†’ **multicollinearity** (redundant information).\n",
    "- Used to **capture feature relationships** and **reduce dimensionality** (e.g., PCA).\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Example (3 features)\n",
    "\\[\n",
    "\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1,X_2) & \\text{Cov}(X_1,X_3) \\\\\n",
    "\\text{Cov}(X_2,X_1) & \\text{Var}(X_2) & \\text{Cov}(X_2,X_3) \\\\\n",
    "\\text{Cov}(X_3,X_1) & \\text{Cov}(X_3,X_2) & \\text{Var}(X_3)\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "This matrix is **symmetric**, since \\( \\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¬ Why Covariance Matters in Machine Learning\n",
    "- **Principal Component Analysis (PCA):**  \n",
    "  Eigenvectors of the covariance matrix define new orthogonal feature directions (principal components).\n",
    "- **Multivariate Gaussian Modeling:**  \n",
    "  Covariance matrix defines **shape and orientation** of the Gaussian distribution in feature space.\n",
    "- **Portfolio Optimization / Risk Analysis:**  \n",
    "  Covariance quantifies **joint variability** between assets/features.\n",
    "- **Feature Selection:**  \n",
    "  Identifies correlated features â†’ helps reduce redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ Key Points to Remember\n",
    "| Concept | Description |\n",
    "|----------|-------------|\n",
    "| **Covariance** | Measures joint variability between two variables |\n",
    "| **Covariance Matrix** | Generalization for multiple variables |\n",
    "| **Symmetric** | \\( \\Sigma_{ij} = \\Sigma_{ji} \\) |\n",
    "| **Diagonal entries** | Represent individual feature variances |\n",
    "| **Used in** | PCA, Gaussian models, feature correlation analysis |\n",
    "| **Unit-dependent** | Not normalized; affected by feature scale |\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Limitations\n",
    "- **Scale-sensitive:** If one feature is in meters and another in millimeters, covariance values can mislead â€” hence **standardization** is often required.  \n",
    "- **Interpretation complexity:** Covariance gives direction but not magnitude of relationship (for that, use **correlation**).  \n",
    "- **Multicollinearity issues:** Highly correlated features can cause instability in regression models.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ’¡ **In short:**  \n",
    "> Covariance measures *how features move together*.  \n",
    "> The **covariance matrix** is its multidimensional extension â€” a cornerstone in **PCA**, **Gaussian models**, and **feature correlation analysis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406e2ee-33d6-467d-b301-1d6d19bf4f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (100ml_env)",
   "language": "python",
   "name": "100ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
