{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f712756",
   "metadata": {},
   "source": [
    "\n",
    "# üßÆ Classification Metrics ‚Äî Understanding Model Evaluation\n",
    "\n",
    "This notebook walks through key **classification metrics** used to evaluate machine learning models.  \n",
    "Each section explains the concept, formula, intuition, and potential pitfalls ‚Äî with examples and diagrams you can visualize later.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Classification Metrics Overview\n",
    "\n",
    "When evaluating a classification model, accuracy alone is rarely enough.  \n",
    "We use **multiple metrics** to understand different aspects of model performance ‚Äî especially when dealing with **imbalanced data** or **unequal costs of errors**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Accuracy\n",
    "\n",
    "**Definition:**  \n",
    "Accuracy measures how often the model correctly predicts the class.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- **TP (True Positive)**: Model predicted positive, and it was positive.  \n",
    "- **TN (True Negative)**: Model predicted negative, and it was negative.  \n",
    "- **FP (False Positive)**: Model predicted positive, but it was negative.  \n",
    "- **FN (False Negative)**: Model predicted negative, but it was positive.\n",
    "\n",
    "‚úÖ **Good for:** balanced datasets  \n",
    "‚ö†Ô∏è **Not reliable for:** imbalanced datasets (e.g., fraud detection, disease diagnosis)\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Issues with Accuracy\n",
    "\n",
    "Accuracy fails when the dataset is **highly imbalanced**.  \n",
    "Example: In a dataset with 99% negatives and 1% positives, a model that always predicts \"negative\" achieves **99% accuracy** but **0% usefulness**.\n",
    "\n",
    "**Better alternatives:** Precision, Recall, F1/F2 Scores, ROC-AUC.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** summarizes classification performance in a 2√ó2 grid:\n",
    "\n",
    "|               | **Predicted Positive** | **Predicted Negative** |\n",
    "|----------------|------------------------|------------------------|\n",
    "| **Actual Positive** | TP (True Positive) | FN (False Negative) |\n",
    "| **Actual Negative** | FP (False Positive) | TN (True Negative) |\n",
    "\n",
    "It helps identify which types of errors your model is making.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Type I and Type II Errors\n",
    "\n",
    "| Error Type | Description | Example |\n",
    "|-------------|--------------|----------|\n",
    "| **Type I Error (False Positive)** | Predicting positive when it‚Äôs actually negative. | Predicting someone has a disease when they don‚Äôt. |\n",
    "| **Type II Error (False Negative)** | Predicting negative when it‚Äôs actually positive. | Predicting someone is healthy when they actually have the disease. |\n",
    "\n",
    "üí° *Tradeoff:* Reducing one often increases the other ‚Äî depending on the decision threshold.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Precision and Recall\n",
    "\n",
    "### Precision\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "- Out of all predicted positives, how many were correct?  \n",
    "- **High precision** ‚Üí few false positives.\n",
    "\n",
    "### Recall (Sensitivity / True Positive Rate)\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- Out of all actual positives, how many were detected?  \n",
    "- **High recall** ‚Üí few false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ F1, F2, and FŒ≤ Scores\n",
    "\n",
    "F-scores combine precision and recall into one metric using a harmonic mean.\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{(\\beta^2 \\cdot Precision) + Recall}\n",
    "$$\n",
    "\n",
    "- **F1 Score:** balances precision and recall equally (Œ≤=1).  \n",
    "- **F2 Score:** gives **more weight to recall** ‚Äî useful in medical or fraud detection contexts.  \n",
    "- **F0.5 Score:** gives **more weight to precision** ‚Äî useful in spam or recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ ROC Curve and AUC\n",
    "\n",
    "**ROC (Receiver Operating Characteristic)** curve plots:\n",
    "- **True Positive Rate (Recall)** on Y-axis  \n",
    "- **False Positive Rate (1 - Specificity)** on X-axis\n",
    "\n",
    "The **AUC (Area Under Curve)** measures how well the model separates classes.  \n",
    "- **AUC = 1.0:** perfect model  \n",
    "- **AUC = 0.5:** random guessing\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Precision‚ÄìRecall Curve\n",
    "\n",
    "When data is **highly imbalanced**, the **PR curve** is often more informative than ROC.  \n",
    "It shows the trade-off between **precision and recall** for different thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Specificity (True Negative Rate)\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "How well the model identifies **negatives** correctly.  \n",
    "Used alongside recall (sensitivity) in medical testing.\n",
    "\n",
    "---\n",
    "\n",
    "## 11Ô∏è‚É£ Balanced Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Balanced Accuracy} = \\frac{Recall_{positive} + Recall_{negative}}{2}\n",
    "$$\n",
    "\n",
    "Useful when data is imbalanced, as it averages performance across both classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 12Ô∏è‚É£ Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "MCC is a **single-number summary** that works even on imbalanced datasets.\n",
    "\n",
    "$$\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "$$\n",
    "\n",
    "Values range from -1 (total disagreement) to +1 (perfect prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## 13Ô∏è‚É£ Cohen‚Äôs Kappa\n",
    "\n",
    "Cohen‚Äôs Kappa adjusts accuracy for chance agreement.\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "$$\n",
    "\n",
    "- \\( p_o \\): observed accuracy  \n",
    "- \\( p_e \\): expected accuracy by random chance\n",
    "\n",
    "A higher Kappa indicates better performance beyond randomness.\n",
    "\n",
    "---\n",
    "\n",
    "## 14Ô∏è‚É£ Summary Table\n",
    "\n",
    "| Metric | Focus | Best Used When |\n",
    "|---------|--------|----------------|\n",
    "| **Accuracy** | Overall correctness | Classes are balanced |\n",
    "| **Precision** | False positives matter | Spam detection |\n",
    "| **Recall** | False negatives matter | Medical or fraud detection |\n",
    "| **F1/F2** | Trade-off metric | General evaluation |\n",
    "| **ROC-AUC** | Ranking power | Balanced or large datasets |\n",
    "| **PR-AUC** | Minority detection | Imbalanced datasets |\n",
    "| **MCC** | Correlation-based | Any dataset size |\n",
    "| **Kappa** | Adjusted accuracy | Chance-corrected evaluation |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Takeaways\n",
    "\n",
    "- **Always look beyond accuracy.**  \n",
    "- Choose metrics based on **business impact** of errors.  \n",
    "- Tune **thresholds** and **monitor multiple metrics** to ensure robust model evaluation.  \n",
    "- For imbalanced datasets, use **Precision, Recall, F-score, PR-AUC, and MCC** over plain accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "üìò *Next Step:*  \n",
    "Add visuals like ROC and PR curves using scikit-learn‚Äôs `plot_roc_curve` and `plot_precision_recall_curve`, and create confusion matrices with `ConfusionMatrixDisplay`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
