{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ee7343-4572-4d31-a77e-8b9a21deb5af",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Feature Engineering in Machine Learning\n",
    "\n",
    "### üß† What is Feature Engineering?\n",
    "**Feature Engineering** is the process of using domain knowledge to **extract, transform, and select meaningful features** from raw data to improve the performance of machine learning models.\n",
    "\n",
    "It bridges the gap between **raw data** and **predictive insight** ‚Äî shaping data into forms that models can learn from effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Key Components of Feature Engineering\n",
    "\n",
    "#### 1. Feature Transformation\n",
    "Transforming existing features to make them more suitable for algorithms.\n",
    "\n",
    "- **Normalization / Scaling:** Bring all numerical columns to the same range  \n",
    "  _(e.g., Min-Max Scaling, Standardization (Z-score))_\n",
    "- **Log Transform / Box-Cox:** Reduce skewness in data.\n",
    "- **Binning / Discretization:** Convert continuous values into categorical bins.\n",
    "- **Encoding Categorical Features:**  \n",
    "  - One-Hot Encoding  \n",
    "  - Label Encoding  \n",
    "  - Target / Frequency Encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Feature Construction\n",
    "Creating **new features** from existing ones to better represent the underlying patterns.\n",
    "\n",
    "- Polynomial features  \n",
    "- Ratios, differences, or interaction terms between variables  \n",
    "- Date-time features (year, month, day, weekday, time delta, etc.)  \n",
    "- Domain-specific logic (e.g., conversion rates, ratios, flags)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Feature Selection\n",
    "Reducing dimensionality by keeping only **relevant features** and removing noisy or redundant ones.\n",
    "\n",
    "- **Filter Methods:** Correlation, Chi-square test, ANOVA  \n",
    "- **Wrapper Methods:** Recursive Feature Elimination (RFE)  \n",
    "- **Embedded Methods:** Lasso (L1), Ridge (L2), Decision Tree feature importance  \n",
    "- Benefits: Improves performance, reduces overfitting, and speeds up training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Feature Extraction\n",
    "Deriving new compact representations from existing data (usually high-dimensional).\n",
    "\n",
    "- **PCA (Principal Component Analysis)** ‚Äì reduces correlated features into uncorrelated components.  \n",
    "- **t-SNE / UMAP** ‚Äì for visualization and non-linear dimensionality reduction.  \n",
    "- **Autoencoders** ‚Äì deep learning approach to learn compressed feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Handling Common Data Issues\n",
    "\n",
    "#### üß© Missing Value Imputation\n",
    "- Replace missing values with **Mean, Median, or Mode** (depending on data type).  \n",
    "- Use special placeholders like `\"Unknown\"` or `0` when appropriate.  \n",
    "- Advanced: **KNN imputation**, **Iterative Imputer**, or **model-based imputation**.\n",
    "\n",
    "#### üé≠ Handling Categorical Features\n",
    "- One-Hot Encoding (for nominal data)  \n",
    "- Ordinal Encoding (for ordered categories)  \n",
    "- Frequency Encoding (for high-cardinality features)\n",
    "\n",
    "#### üö® Outlier Detection\n",
    "Outliers can heavily affect model performance.\n",
    "- Techniques:\n",
    "  - Z-score, IQR method, Isolation Forest, or DBSCAN  \n",
    "- Decision point:\n",
    "  - **Keep** outliers if they represent real, meaningful variations.  \n",
    "  - **Remove** them if they result from data errors or noise.\n",
    "\n",
    "#### üìè Feature Scaling\n",
    "Ensures all numeric features contribute equally during model training.\n",
    "- **Standardization:** \\( x' = \\frac{x - \\mu}{\\sigma} \\)\n",
    "- **Normalization:** \\( x' = \\frac{x - x_{min}}{x_{max} - x_{min}} \\)\n",
    "- **Robust Scaling:** Scales using median and IQR (useful with outliers).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Advanced Feature Engineering Techniques\n",
    "\n",
    "- **Polynomial Feature Expansion**\n",
    "- **Target Encoding for Categorical Variables**\n",
    "- **Interaction Features (Feature Crosses)**\n",
    "- **Feature Hashing (for text or high-cardinality data)**\n",
    "- **Text Vectorization (TF-IDF, Word2Vec, BERT embeddings)**\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Summary\n",
    "\n",
    "| Stage | Purpose | Example |\n",
    "|--------|----------|----------|\n",
    "| Transformation | Modify existing features | Log scaling, Encoding |\n",
    "| Construction | Create new features | Ratios, time-based features |\n",
    "| Selection | Choose the best subset | Correlation, Lasso |\n",
    "| Extraction | Compress information | PCA, Autoencoders |\n",
    "\n",
    "Feature engineering is **where data science meets intuition** ‚Äî it‚Äôs less about algorithms and more about understanding your data.\n",
    "\n",
    "> ‚ÄúBetter data beats fancier algorithms.‚Äù ‚Äî Andrew Ng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eacce1-8236-4817-a734-eb6d70bc2ee9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (100ml_env)",
   "language": "python",
   "name": "100ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
